---
description:
globs:
alwaysApply: false
---
# RAG QA ì±—ë´‡ êµ¬í˜„ ì˜ˆì‹œ

## ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¡° ([main.py](mdc:main.py))

### í•„ìˆ˜ ì„í¬íŠ¸
```python
import os
import gradio as gr
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

### í™˜ê²½ ì„¤ì •
```python
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# ìƒìˆ˜ ì •ì˜
DATA_DIR = "data/extracted"
VECTORSTORE_DIR = "vectorstore"
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
```

## ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ ì˜ˆì‹œ

### URL ì½˜í…ì¸  ì¶”ì¶œ í•¨ìˆ˜
```python
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def fetch_url_content(url: str) -> str:
    """URLì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
        for script in soup(["script", "style"]):
            script.decompose()
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = soup.get_text()
        
        # ì¤„ë°”ê¿ˆ ì •ë¦¬
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        print(f"URL ì½˜í…ì¸  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return ""
```

### ë°ì´í„° ì €ì¥ í•¨ìˆ˜
```python
def save_content_to_file(content: str, filename: str) -> bool:
    """ì¶”ì¶œëœ ì½˜í…ì¸ ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        os.makedirs(DATA_DIR, exist_ok=True)
        filepath = os.path.join(DATA_DIR, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"ì½˜í…ì¸ ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False
```

## RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„

### ë²¡í„° ì €ì¥ì†Œ ìƒì„±
```python
def create_vectorstore(data_dir: str) -> FAISS:
    """í…ìŠ¤íŠ¸ íŒŒì¼ë“¤ë¡œë¶€í„° FAISS ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        # ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ
        documents = []
        for filename in os.listdir(data_dir):
            if filename.endswith('.txt'):
                filepath = os.path.join(data_dir, filename)
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    documents.append({
                        'content': content,
                        'source': filename
                    })
        
        if not documents:
            raise ValueError("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        
        texts = []
        metadatas = []
        for doc in documents:
            chunks = text_splitter.split_text(doc['content'])
            texts.extend(chunks)
            metadatas.extend([{'source': doc['source']} for _ in chunks])
        
        # ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = FAISS.from_texts(texts, embeddings, metadatas)
        
        # ë²¡í„° ì €ì¥ì†Œ ì €ì¥
        os.makedirs(VECTORSTORE_DIR, exist_ok=True)
        vectorstore.save_local(VECTORSTORE_DIR)
        
        return vectorstore
    
    except Exception as e:
        print(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
```

### QA ì²´ì¸ ì„¤ì •
```python
def setup_qa_chain(vectorstore: FAISS) -> RetrievalQA:
    """QA ì²´ì¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    try:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0,
            openai_api_key=OPENAI_API_KEY
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=True
        )
        
        return qa_chain
    
    except Exception as e:
        print(f"QA ì²´ì¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        return None
```

## Gradio UI êµ¬í˜„

### ë‹µë³€ ìƒì„± í•¨ìˆ˜
```python
def answer_question(question: str, qa_chain: RetrievalQA) -> tuple[str, str]:
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        if not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", ""
        
        result = qa_chain({"query": question})
        answer = result["result"]
        
        # ì°¸ê³  ë¬¸ì„œ ì •ë³´
        sources = []
        for doc in result["source_documents"]:
            source = doc.metadata.get("source", "Unknown")
            content_preview = doc.page_content[:100] + "..."
            sources.append(f"ğŸ“„ {source}: {content_preview}")
        
        source_info = "\n\n**ì°¸ê³  ë¬¸ì„œ:**\n" + "\n".join(sources) if sources else ""
        
        return answer, source_info
    
    except Exception as e:
        return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", ""
```

### Gradio ì¸í„°í˜ì´ìŠ¤
```python
def create_gradio_interface(qa_chain: RetrievalQA):
    """Gradio ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    def chat_function(message, history):
        answer, sources = answer_question(message, qa_chain)
        full_response = answer + sources
        return full_response
    
    # Gradio ChatInterface ì‚¬ìš©
    interface = gr.ChatInterface(
        fn=chat_function,
        title="ğŸ¤– RAG QA ì±—ë´‡",
        description="ì—…ë¡œë“œëœ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!",
        examples=[
            "ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.",
            "í•µì‹¬ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "êµ¬ì²´ì ì¸ ì„¤ëª…ì„ í•´ì£¼ì„¸ìš”."
        ],
        theme=gr.themes.Soft()
    )
    
    return interface
```

## ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜

### ì „ì²´ í”Œë¡œìš° í†µí•©
```python
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("RAG QA ì±—ë´‡ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # 1. í™˜ê²½ë³€ìˆ˜ í™•ì¸
    if not OPENAI_API_KEY:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ë˜ëŠ” ìƒì„±
    vectorstore_path = os.path.join(VECTORSTORE_DIR, "index.faiss")
    
    if os.path.exists(vectorstore_path):
        print("âœ… ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = FAISS.load_local(VECTORSTORE_DIR, embeddings)
    else:
        print("ğŸ”„ ìƒˆë¡œìš´ ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        vectorstore = create_vectorstore(DATA_DIR)
    
    if not vectorstore:
        print("âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 3. QA ì²´ì¸ ì„¤ì •
    qa_chain = setup_qa_chain(vectorstore)
    if not qa_chain:
        print("âŒ QA ì²´ì¸ ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return
    
    # 4. Gradio ì¸í„°í˜ì´ìŠ¤ ì‹¤í–‰
    interface = create_gradio_interface(qa_chain)
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )

if __name__ == "__main__":
    main()
```

## ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤

### ë²¡í„° ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
```python
def update_vectorstore(new_url: str):
    """ìƒˆë¡œìš´ URL ì½˜í…ì¸ ë¡œ ë²¡í„° ì €ì¥ì†Œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    content = fetch_url_content(new_url)
    if content:
        filename = f"content_{len(os.listdir(DATA_DIR)) + 1}.txt"
        if save_content_to_file(content, filename):
            # ë²¡í„° ì €ì¥ì†Œ ì¬ìƒì„±
            vectorstore = create_vectorstore(DATA_DIR)
            print("âœ… ë²¡í„° ì €ì¥ì†Œê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
    return False
```
